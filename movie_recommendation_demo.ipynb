{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7be6f33",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running on Colab)\n",
    "# !pip install recommenders\n",
    "# !pip install pyspark\n",
    "# !apt-get update -qq\n",
    "# !apt-get install openjdk-8-jdk-headless -qq\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e097f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
    "\n",
    "# Recommenders library imports\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k\n",
    "from recommenders.evaluation.spark_evaluation import SparkRankingEvaluation\n",
    "from recommenders.models.sar import SAR\n",
    "from recommenders.utils.spark_utils import start_or_get_spark\n",
    "from recommenders.utils.timer import Timer\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Spark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b184321",
   "metadata": {},
   "source": [
    "## Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top K items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# MovieLens data size\n",
    "MOVIELENS_DATA_SIZE = '20m'\n",
    "\n",
    "# Column names\n",
    "COL_USER = \"UserId\"\n",
    "COL_ITEM = \"MovieId\"\n",
    "COL_RATING = \"Rating\"\n",
    "COL_TIMESTAMP = \"Timestamp\"\n",
    "COL_PREDICTION = \"prediction\"\n",
    "\n",
    "# Train/test split ratio\n",
    "TRAIN_RATIO = 0.75\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Dataset: MovieLens-{MOVIELENS_DATA_SIZE}\")\n",
    "print(f\"  Top-K: {TOP_K}\")\n",
    "print(f\"  Train ratio: {TRAIN_RATIO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cda35a",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Python-based models (Popularity, Item-KNN)\n",
    "print(\"Loading MovieLens data...\")\n",
    "df = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[COL_USER, COL_ITEM, COL_RATING, COL_TIMESTAMP]\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of users: {df[COL_USER].nunique()}\")\n",
    "print(f\"Number of items: {df[COL_ITEM].nunique()}\")\n",
    "print(f\"Number of ratings: {len(df)}\")\n",
    "print(f\"Rating density: {len(df) / (df[COL_USER].nunique() * df[COL_ITEM].nunique()):.4%}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d580f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for Python-based models\n",
    "train_df, test_df = python_random_split(df, ratio=TRAIN_RATIO, seed=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train set: {len(train_df)} ratings\")\n",
    "print(f\"Test set: {len(test_df)} ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce9db3",
   "metadata": {},
   "source": [
    "## 2. Algorithm 1: Popularity-Based Recommender\n",
    "\n",
    "This recommender suggests the most popular items based on rating counts and average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432da9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityRecommender:\n",
    "    \"\"\"Popularity-based recommender system.\"\"\"\n",
    "    \n",
    "    def __init__(self, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING):\n",
    "        self.col_user = col_user\n",
    "        self.col_item = col_item\n",
    "        self.col_rating = col_rating\n",
    "        self.popular_items = None\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        \"\"\"Calculate item popularity based on rating count and average rating.\"\"\"\n",
    "        # Calculate popularity score: weighted combination of count and avg rating\n",
    "        item_stats = train_df.groupby(self.col_item).agg({\n",
    "            self.col_rating: ['count', 'mean']\n",
    "        })\n",
    "        item_stats.columns = ['count', 'avg_rating']\n",
    "        \n",
    "        # Normalize and combine metrics\n",
    "        item_stats['count_norm'] = (item_stats['count'] - item_stats['count'].min()) / \\\n",
    "                                    (item_stats['count'].max() - item_stats['count'].min())\n",
    "        item_stats['rating_norm'] = (item_stats['avg_rating'] - item_stats['avg_rating'].min()) / \\\n",
    "                                     (item_stats['avg_rating'].max() - item_stats['avg_rating'].min())\n",
    "        \n",
    "        # Popularity score: 70% count, 30% rating\n",
    "        item_stats['popularity_score'] = 0.7 * item_stats['count_norm'] + 0.3 * item_stats['rating_norm']\n",
    "        \n",
    "        self.popular_items = item_stats.sort_values('popularity_score', ascending=False)\n",
    "        return self\n",
    "    \n",
    "    def recommend_k_items(self, test_df, top_k=TOP_K, remove_seen=True):\n",
    "        \"\"\"Recommend top-k popular items for each user.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        for user_id in test_df[self.col_user].unique():\n",
    "            # Get user's seen items if we need to remove them\n",
    "            if remove_seen:\n",
    "                seen_items = test_df[test_df[self.col_user] == user_id][self.col_item].values\n",
    "                available_items = self.popular_items[~self.popular_items.index.isin(seen_items)]\n",
    "            else:\n",
    "                available_items = self.popular_items\n",
    "            \n",
    "            # Recommend top-k items\n",
    "            top_items = available_items.head(top_k)\n",
    "            \n",
    "            for rank, (item_id, row) in enumerate(top_items.iterrows(), 1):\n",
    "                recommendations.append({\n",
    "                    self.col_user: user_id,\n",
    "                    self.col_item: item_id,\n",
    "                    COL_PREDICTION: row['popularity_score']\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(recommendations)\n",
    "\n",
    "print(\"Popularity-based recommender defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Popularity-based model\n",
    "print(\"Training Popularity-based recommender...\")\n",
    "with Timer() as train_time:\n",
    "    popularity_model = PopularityRecommender()\n",
    "    popularity_model.fit(train_df)\n",
    "\n",
    "print(f\"Training completed in {train_time.interval:.2f} seconds\")\n",
    "print(f\"\\nTop 10 most popular movies:\")\n",
    "print(popularity_model.popular_items.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for Popularity model\n",
    "print(\"Generating recommendations...\")\n",
    "with Timer() as pred_time:\n",
    "    popularity_predictions = popularity_model.recommend_k_items(test_df, top_k=TOP_K, remove_seen=True)\n",
    "\n",
    "print(f\"Prediction completed in {pred_time.interval:.2f} seconds\")\n",
    "print(f\"Generated {len(popularity_predictions)} recommendations\")\n",
    "popularity_predictions.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57764b6f",
   "metadata": {},
   "source": [
    "## 3. Algorithm 2: Item-KNN (SAR - Smart Adaptive Recommendations)\n",
    "\n",
    "Using the SAR algorithm from the recommenders library, which is an item-based collaborative filtering approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eabdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SAR (Item-based CF) model\n",
    "print(\"Training SAR (Item-KNN) recommender...\")\n",
    "\n",
    "sar_model = SAR(\n",
    "    col_user=COL_USER,\n",
    "    col_item=COL_ITEM,\n",
    "    col_rating=COL_RATING,\n",
    "    col_timestamp=COL_TIMESTAMP,\n",
    "    similarity_type=\"jaccard\",  # Can also use 'lift' or 'cooccurrence'\n",
    "    time_decay_coefficient=30,\n",
    "    timedecay_formula=True,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "with Timer() as train_time:\n",
    "    sar_model.fit(train_df)\n",
    "\n",
    "print(f\"Training completed in {train_time.interval:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for SAR model\n",
    "print(\"Generating recommendations...\")\n",
    "with Timer() as pred_time:\n",
    "    sar_predictions = sar_model.recommend_k_items(\n",
    "        test_df,\n",
    "        top_k=TOP_K,\n",
    "        remove_seen=True\n",
    "    )\n",
    "\n",
    "print(f\"Prediction completed in {pred_time.interval:.2f} seconds\")\n",
    "print(f\"Generated {len(sar_predictions)} recommendations\")\n",
    "sar_predictions.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020de86",
   "metadata": {},
   "source": [
    "## 4. Algorithm 3: ALS (Alternating Least Squares)\n",
    "\n",
    "Matrix factorization using PySpark's ALS implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a77475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "spark = start_or_get_spark(\"Movie Recommendation Demo\", memory=\"8g\")\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "print(\"Spark session initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Spark\n",
    "schema = StructType(\n",
    "    (\n",
    "        StructField(COL_USER, IntegerType()),\n",
    "        StructField(COL_ITEM, IntegerType()),\n",
    "        StructField(COL_RATING, FloatType()),\n",
    "        StructField(COL_TIMESTAMP, LongType()),\n",
    "    )\n",
    ")\n",
    "\n",
    "data = movielens.load_spark_df(spark, size=MOVIELENS_DATA_SIZE, schema=schema)\n",
    "train_spark, test_spark = spark_random_split(data, ratio=TRAIN_RATIO, seed=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train set: {train_spark.count()} ratings\")\n",
    "print(f\"Test set: {test_spark.count()} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e709f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ALS model\n",
    "print(\"Training ALS recommender...\")\n",
    "\n",
    "als_model = ALS(\n",
    "    rank=10,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    regParam=0.05,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=False,\n",
    "    seed=RANDOM_SEED,\n",
    "    userCol=COL_USER,\n",
    "    itemCol=COL_ITEM,\n",
    "    ratingCol=COL_RATING\n",
    ")\n",
    "\n",
    "with Timer() as train_time:\n",
    "    als_fitted_model = als_model.fit(train_spark)\n",
    "\n",
    "print(f\"Training completed in {train_time.interval:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for ALS model\n",
    "print(\"Generating recommendations...\")\n",
    "\n",
    "with Timer() as pred_time:\n",
    "    # Get all user-item pairs\n",
    "    users = train_spark.select(COL_USER).distinct()\n",
    "    items = train_spark.select(COL_ITEM).distinct()\n",
    "    user_item = users.crossJoin(items)\n",
    "    \n",
    "    # Get predictions\n",
    "    dfs_pred = als_fitted_model.transform(user_item)\n",
    "    \n",
    "    # Remove seen items\n",
    "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "        train_spark.alias(\"train\"),\n",
    "        (dfs_pred[COL_USER] == train_spark[COL_USER]) & \n",
    "        (dfs_pred[COL_ITEM] == train_spark[COL_ITEM]),\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    als_predictions = dfs_pred_exclude_train.filter(\n",
    "        dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()\n",
    "    ).select(\n",
    "        'pred.' + COL_USER, \n",
    "        'pred.' + COL_ITEM, \n",
    "        'pred.' + COL_PREDICTION\n",
    "    )\n",
    "    \n",
    "    # Force execution\n",
    "    als_predictions.cache().count()\n",
    "\n",
    "print(f\"Prediction completed in {pred_time.interval:.2f} seconds\")\n",
    "als_predictions.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1312473",
   "metadata": {},
   "source": [
    "## 5. Evaluation: MAP@K and NDCG@K\n",
    "\n",
    "Now let's evaluate all three algorithms using ranking metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Popularity-based model\n",
    "print(\"Evaluating Popularity-based Recommender...\")\n",
    "\n",
    "popularity_map = map_at_k(\n",
    "    test_df, \n",
    "    popularity_predictions, \n",
    "    col_user=COL_USER, \n",
    "    col_item=COL_ITEM,\n",
    "    col_rating=COL_RATING,\n",
    "    col_prediction=COL_PREDICTION,\n",
    "    k=TOP_K\n",
    ")\n",
    "\n",
    "popularity_ndcg = ndcg_at_k(\n",
    "    test_df, \n",
    "    popularity_predictions, \n",
    "    col_user=COL_USER, \n",
    "    col_item=COL_ITEM,\n",
    "    col_rating=COL_RATING,\n",
    "    col_prediction=COL_PREDICTION,\n",
    "    k=TOP_K\n",
    ")\n",
    "\n",
    "print(f\"Popularity - MAP@{TOP_K}: {popularity_map:.4f}\")\n",
    "print(f\"Popularity - NDCG@{TOP_K}: {popularity_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SAR (Item-KNN) model\n",
    "print(\"Evaluating SAR (Item-KNN) Recommender...\")\n",
    "\n",
    "sar_map = map_at_k(\n",
    "    test_df, \n",
    "    sar_predictions, \n",
    "    col_user=COL_USER, \n",
    "    col_item=COL_ITEM,\n",
    "    col_rating=COL_RATING,\n",
    "    col_prediction=COL_PREDICTION,\n",
    "    k=TOP_K\n",
    ")\n",
    "\n",
    "sar_ndcg = ndcg_at_k(\n",
    "    test_df, \n",
    "    sar_predictions, \n",
    "    col_user=COL_USER, \n",
    "    col_item=COL_ITEM,\n",
    "    col_rating=COL_RATING,\n",
    "    col_prediction=COL_PREDICTION,\n",
    "    k=TOP_K\n",
    ")\n",
    "\n",
    "print(f\"SAR - MAP@{TOP_K}: {sar_map:.4f}\")\n",
    "print(f\"SAR - NDCG@{TOP_K}: {sar_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ALS model\n",
    "print(\"Evaluating ALS Recommender...\")\n",
    "\n",
    "als_eval = SparkRankingEvaluation(\n",
    "    test_spark, \n",
    "    als_predictions,\n",
    "    k=TOP_K,\n",
    "    col_user=COL_USER,\n",
    "    col_item=COL_ITEM,\n",
    "    col_rating=COL_RATING,\n",
    "    col_prediction=COL_PREDICTION,\n",
    "    relevancy_method=\"top_k\"\n",
    ")\n",
    "\n",
    "als_map = als_eval.map_at_k()\n",
    "als_ndcg = als_eval.ndcg_at_k()\n",
    "\n",
    "print(f\"ALS - MAP@{TOP_K}: {als_map:.4f}\")\n",
    "print(f\"ALS - NDCG@{TOP_K}: {als_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc92c65",
   "metadata": {},
   "source": [
    "## 6. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results comparison table\n",
    "results = pd.DataFrame({\n",
    "    'Algorithm': ['Popularity', 'Item-KNN (SAR)', 'ALS'],\n",
    "    'MAP@10': [popularity_map, sar_map, als_map],\n",
    "    'NDCG@10': [popularity_ndcg, sar_ndcg, als_ndcg]\n",
    "})\n",
    "\n",
    "# Sort by MAP@10\n",
    "results = results.sort_values('MAP@10', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS - MovieLens 20M Recommendation Algorithms\")\n",
    "print(\"=\"*60)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Highlight best performing model\n",
    "best_model = results.iloc[0]['Algorithm']\n",
    "print(f\"\\nüèÜ Best performing model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b682038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAP@10 comparison\n",
    "axes[0].bar(results['Algorithm'], results['MAP@10'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0].set_ylabel('MAP@10', fontsize=12)\n",
    "axes[0].set_title('Mean Average Precision @ 10', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0, max(results['MAP@10']) * 1.2])\n",
    "for i, v in enumerate(results['MAP@10']):\n",
    "    axes[0].text(i, v + 0.001, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# NDCG@10 comparison\n",
    "axes[1].bar(results['Algorithm'], results['NDCG@10'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1].set_ylabel('NDCG@10', fontsize=12)\n",
    "axes[1].set_title('Normalized Discounted Cumulative Gain @ 10', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0, max(results['NDCG@10']) * 1.2])\n",
    "for i, v in enumerate(results['NDCG@10']):\n",
    "    axes[1].text(i, v + 0.001, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('recommendation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'recommendation_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283486d2",
   "metadata": {},
   "source": [
    "## 7. Sample Recommendations\n",
    "\n",
    "Let's look at some actual recommendations for a sample user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429cf890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample user from test set\n",
    "sample_user = test_df[COL_USER].value_counts().index[0]\n",
    "print(f\"Sample User ID: {sample_user}\")\n",
    "print(f\"\\nUser's actual ratings in test set:\")\n",
    "user_test_ratings = test_df[test_df[COL_USER] == sample_user].sort_values(COL_RATING, ascending=False)\n",
    "print(user_test_ratings.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations from all three models\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RECOMMENDATIONS FOR USER {sample_user}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Popularity recommendations\n",
    "pop_recs = popularity_predictions[popularity_predictions[COL_USER] == sample_user].head(TOP_K)\n",
    "print(\"Popularity-based Recommendations:\")\n",
    "print(pop_recs)\n",
    "\n",
    "# SAR recommendations\n",
    "sar_recs = sar_predictions[sar_predictions[COL_USER] == sample_user].head(TOP_K)\n",
    "print(\"\\nItem-KNN (SAR) Recommendations:\")\n",
    "print(sar_recs)\n",
    "\n",
    "# ALS recommendations\n",
    "als_recs_pd = als_predictions.filter(als_predictions[COL_USER] == sample_user).toPandas().head(TOP_K)\n",
    "print(\"\\nALS Recommendations:\")\n",
    "print(als_recs_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0c69e",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Popularity-based Recommender**: Simple baseline that recommends globally popular items. Fast but lacks personalization.\n",
    "\n",
    "2. **Item-KNN (SAR)**: Item-based collaborative filtering that finds similar items. Good balance of performance and interpretability.\n",
    "\n",
    "3. **ALS**: Matrix factorization approach that learns latent factors. Often provides the best personalization but requires more computational resources.\n",
    "\n",
    "### Metrics:\n",
    "- **MAP@K**: Measures precision of recommendations considering order\n",
    "- **NDCG@K**: Measures ranking quality with position-based discounting\n",
    "\n",
    "The results show the trade-offs between algorithmic complexity, computational cost, and recommendation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Demo complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
